## Goal of this Project

### RAG-Gym's Agentic Loop

The standard RAG-Gym framework operates on an agentic loop where an agent makes decisions in an environment to solve a user's question. Hereâ€™s a concise overview of that flow:

```python
# The process starts with a user's question.
question = "What are the main differences between llamas and alpacas?"

# 1. Initialize the Environment
# The 'env' manages the state and provides access to the knowledge corpus (e.g., Wikipedia).
# It defines how the agent interacts with the data.
env = rag_gym.make(retriever_name="BM25", corpus_name="Wikipedia")

# 2. Initialize the Agent
# The 'agent' contains the LLMs for thinking and acting.
# It has a component for generating actions (Actor) and scoring them (Critic).
agent = rag_gym.ReSearchAgent(llm_name="meta-llama/Meta-Llama-3.1-8B-Instruct")

# 3. Create the Initial State
# The 'observation' holds the initial question and an empty history.
observation, info = env.reset(question=question)

# 4. Start the Actor-Critic Loop
# The agent iterates, taking steps to gather information and formulate an answer.
for i in range(max_iterations):

    # 4a. Actor: Generate candidate actions based on the current state.
    actions = agent.generate_action(state=observation, num_actions=10)
    
    # 4b. Critic: Score each candidate action's usefulness.
    rewards = agent.score(observation, actions)
    
    # 4c. Select the best action based on the critic's score.
    best_action = actions[rewards.index(max(rewards))]
    
    # 4d. Environment: Execute the action (e.g., perform a search).
    # This updates the state with new information (retrieved documents).
    observation, reward, terminated, truncated, info = env.step(best_action)
    
    # If the action was "Finish", the loop ends.
    if terminated or truncated:
        break

# The final answer is typically the result of the "Finish" action.
print(f"Final Answer: {observation.history[-1].action.answer}")
```

-----

We want to construct a custom version of this system where we replace the core components with our own LLM-based logic and use an external RAGFlow instance for all document retrieval.

This system can be conceptualized as having three main, customized components: an **Actor Agent**, a **Critic Agent**, and a **Retrieval Environment**.

### \#\# 1. The "Actor" Agent: Custom Action Generation ðŸ§ 

This component replaces the behavior of `ReSearchAgent.generate_action(...)`.

  * **Objective**: To propose a set of possible next steps (actions) based on the current state of the problem.
  * **Implementation**: We'll use a dedicated **LLM call** for this. The LLM will be prompted with the entire context of the problem-solving process so far.
  * **Input to the LLM**: The prompt will be a structured JSON object containing the full history and the original question. This object provides the complete context for the LLM to make an informed decision.
    ```json
    {
        "question": "What are the main differences between llamas and alpacas?",
        "history": [
            {
                "action": "Search",
                "query": "physical differences between llama and alpaca",
                "document_chunks": [
                    "A chunk of a document describing llama size and ear shape.",
                    "A chunk of a document about alpaca fiber and facial features.",
                    ...
                ]
            },
            ...
        ]
    }
    ```
  * **Output from the LLM**: The LLM's response should be a list of structured action candidates. The valid action types are limited to **`Search(query: str)`** and **`Finish(final_answer: str)`**.
    ```json
    [
      {"action": "Search", "query": "llama vs alpaca temperament"},
      {"action": "Search", "query": "uses of alpaca fiber"},
      {"action": "Finish", "answer": "Llamas are larger with banana-shaped ears, while alpacas are smaller with softer fiber."}
    ]
    ```

-----

### \#\# 2. The "Critic" Agent: Simplified Action Selection âœ…

This component replaces `ReSearchAgent.score(...)` with a simplified selection mechanism. Instead of scoring each action individually, this critic directly chooses the best one from the list provided by the Actor.

  * **Objective**: To evaluate the list of candidate actions from the Actor and select the single most promising one to execute next.
  * **Implementation**: We'll use a second, more analytical **LLM call**. This LLM acts as the rational decision-maker.
  * **Input to the LLM**: The prompt will include the same context as the Actor (question and history) but will also include the **list of candidate actions** generated by the Actor. The LLM will be instructed to "choose the best action from the following list to make progress on the original question."
  * **Output from the LLM**: The LLM's response will be the single, chosen action in a structured format, ready to be executed. For example:
    ```json
    {"action": "Search", "query": "llama vs alpaca temperament"}
    ```
    This approach simplifies the logic by replacing an N-step scoring and ranking process with a single decision-making step.

-----

### \#\# 3. The Retrieval Environment: RAGFlow Integration ðŸ“š

This component customizes the `env.step(...)` function's retrieval logic by connecting it to our external knowledge base.

  * **Objective**: To fetch relevant document chunks from our custom datasets hosted in RAGFlow.
  * **Implementation**: We will integrate our system with a running RAGFlow instance by using its **Python API**.
  * **Workflow**:
    1.  Our Critic Agent selects the best action (e.g., `Search(query: "llama vs alpaca temperament")`).
    2.  Our custom `env.step(...)` function is triggered. It uses the **RAGFlow Python API** to send the `query` to our specified knowledge base.
    3.  RAGFlow performs its optimized retrieval process and returns the most relevant document chunks.
    4.  Our `env.step(...)` function receives these chunks and updates the agent's `observation` state, adding the new action and its retrieved documents to the history. This new information will be used in the next cycle.
